---
title: "surv_proj2"
author: "Lxy"
date: "March 31, 2017"
output: html_document
header includes: \usepackage{amsmath, amsthm, amssymb}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## summary

The first project focuses on employing numerical apporximations from Lindley (1980) and Tierney & Kadane (1986) on survival data which are considered interval-censored. We have considered only a parametric approach assuming the survival data follows a loglogistic distribution model.

For the second project, we will be looking at expanding the scope through the following: 
1. Estimation of the Bayesian parameters with asymmetric loss functions
2. Computational aspects of Bayesian parametric and non-parametric approach
3. Implementation on methods for interval-censored data in R with package Icens and intcox

### Estimation of Bayesian parameters with asymmetric loss functions

Asymmetric loss functions are used to reflect, in most situation of interest, overestimation of a parameter does not produce the same economic consequence as underestimation. both the non-informative prior and informative prior on the reliability level at a prefixed time value are considered.

Bayes approach allows:
* prior information on the parameters of the failure model to be incorporated into the inferential procedure
* considerations on the economic consequences arising from the wrong point estimate to be incorporated into the estimation procedure by using an appropriate loss function

Let \( t_1, t_1, \dots, t_m, t_{m+1}, \dots, t_n\) denote experimental data from a left-truncated exponential distribution, \( f(t) = \theta^{-1}exp[-(t-\eta)/\theta], \ t \geq \eta \geq 0;\ \theta \geq 0  \) collected under a randomly censored sampling with \(m\) items failed at \(t_1, t_2, \dots, t_m\) and \(n-m\) items functioning at \(t_{m+1}, \dots, t_n\).

The likelihood function, give n the sample information is:
\[
l(\eta, \theta) = \theta^{-m} exp[-(X-n\eta)/\theta]
\]
where \(X = \sum_{i=1}^{n} t_i\) is the accumulated observed life

### Posterior distribution
When no prior information is known, the non-informative prior on \(\eta\) and \(\theta\) is
\[g(\eta, \theta)  \propto 1/\theta \]

### Asymmetric loss functions

The loss function \(L(\hat{\alpha}, \alpha)\) provides a measure of financial consequences arising from a wrong estimate \(\hat{\alpha}\) of the unknow quantity \(\alpha\). the choice of the appropriate loss function depends on financial considerations and is independent from the estimation procedure.

The LINEX loss function, introduced by Varian (1975), is one asymmetric loss function which rises approximately exponentially on one side of zero and approximately linearly on the other side.

Under the assumption that the minimal loss occurs at \(\hat{\alpha} = \alpha\), the LINEX loss function can be expressed as:
\[
L_L(\tilde{\alpha}, \alpha) \propto exp[q(\tilde{\alpha}-\alpha)] - q(\tilde{\alpha}-\alpha) - 1, \qquad q \neq 0
\]
The sign of shape parameter \(q\) reflects the direction of the asymmetry, \(q>0\) if overestimation is more serious than underestimation. Magnitude of \(q\) reflects the degree of asymmetry. For small \(|q|\), the LINEX loss function is almost symmetric and nearly quadratic. The Bayes estimate under such a loss, say \(\hat{\alpha}^L\), is not far from the posterior mean.

The posterior expectation of the LINEX loss function is 
\[
E_{\alpha}\{ L_L(\tilde{\alpha}, \alpha)\} \propto exp(q\tilde{\alpha})\ E_{\alpha} \{ exp(-q\alpha)\} - q(\tilde{\alpha} - E_\alpha\{\alpha\}) - 1
\]
where \(E_\alpha \{ f(\alpha)\}\) denotes the expectation of \(f(\alpha)\) with respect to the posterior density \(\pi(\alpha | data)\). The Bayes estimate \(\tilde{\alpha}\) that minimises the expectation above is 
\[
\tilde{\alpha}^L = - \frac{1}{q} ln \big( E_\alpha \{ exp(-q\alpha)\}\big)
\]
provided that \(E_\alpha \{ exp(-q\alpha)\) exists and is finite, Zellner (1986).

Although the LINEX loss function is popular for the estimation of the location parameter, it appears to be not suitable for the estimation of the scale parameter and other quantities (Basu & Ebrahimi (1991) and Parsian & Sanjani Farsipour (1993)). Basu & Ebrahimi suggested the Modified LINEX loss:
\[
L_M (\tilde{\alpha}, \alpha) \propto exp[q^\prime (\tilde{\alpha}/\alpha-1)] - q^\prime (\tilde{\alpha}/ \alpha -1) - 1 ]
\]
and \(\tilde{\alpha}^M\) that minimises the modified loss function, is the solution of:
\[
E_\alpha\{ \alpha^{-1} exp(q^\prime \tilde{\alpha}^{\Lambda I} / \alpha) \} = exp(q^\prime) E_\alpha \{1/\alpha\}
\]
povided that all the expectations are finite.

The Modified LINEX loss appears to be appropriate in the context of the stimation of \(\theta, \lambda, \rho, R(t_0) \) and \(t_{R_0}\). However the expectations  in the modified loss function cannot be solved analytically with respect to the posterior distributions. Thus \(\tilde{\alpha}^M\) cannot be given in a closed form.

Another alternative to the Modififed LINEX is the General Entropy loss proposed by Calabria and Pulcini (1994a):
\[
L_E (\tilde{\alpha}, \alpha) \propto (\tilde{\alpha}/\alpha)^p - p\ ln(\tilde{\alpha}/\alpha) -1
\]
where the minimm occurs at \(\tilde{\alpha} = \alpha\)

When \(p>0\), a postive error \( (\tilde\alpha > \alpha) \) causes more serious consequences than a negative error. The Bayes estimate of \(\alpha\) under the General Entropy loss is in a closed form:
\[
\tilde\alpha^E = [E_\alpha (\alpha^{-p})^{-1/p}]
\]
provided that \(E_\alpha(\alpha^{-p})\) exists and is finite.
Some notes on the estimate above:
* When \(p=1\), the Bayes estimate conicides with the Bayes estimate under the weighted squared-error loss function \( (\tilde\alpha-\alpha)^2/ \alpha\)
* When \( p=-1 \), the Bayes estimate coincides with the Bayes estimate under the Squared Error loss function

To choose the appropriate value of the shape parameter of the selected loss function so that the asymmetry of the loss function reflects the practical situation, the following is proposed:
* when the selected function is the LINEX loss, the value \(q\) is chosen to satisfy
\[
\frac{L_L(\alpha + \Delta, \alpha)}{L_L(\alpha - \Delta, \alpha)} = r, \quad \Delta > 0
\]
where \(r\) is the value of the ratio of the loss for overestimation and the loss for underestimation.

* If the selected function is the Modified LINEX loss or the General Entrophy loss, then we search for the value \(q\prime\) or \(p\) that satisfies
\[
\frac{L(\alpha \cdot \delta, \alpha)}{L_L(\alpha/\delta, \alpha)} = r, \quad \delta > 1
\]
where the \(r\) is the value of the ratio of the loss for an overestimation of \(\delta\) times and the loss for an underestimation of \(1/\delta\) times.

### Comparison of Bayes estimates

Taking expectation over all possible outcomes of the experiment with the unknown random loss \(L(\tilde\alpha, \alpha)\) , the risk function of \(\tilde\alpha\) can be obtained as
\[
RF(\tilde\alpha) = E\{L(\tilde\alpha, \alpha\} = \int L(\tilde\alpha, \alpha) \cdot f(\tilde\alpha)\ d\tilde\alpha
\]
The risk function can be used as an measure of robustness of the Bayes procedure with respect to a wrong choice of the prior distribution or for comparison against classical estimators, i.e. Maximum Likelihood (ML).

The Bayes estimator can be compared to ML estimators in terms of "Relative Efficiency", defined as the raio of the risk function of the ML estimator to the risk function of the corresponding Bayes estimator. Relative Efficiencies greater than 1 are favourable to the Bayes procedure.

Based on the simulation results by Calabria & Pulcini (1996), Bayes estimators based on both non-informative or informative priors are more efficient than ML ones. The result regarding non-informative procedure is expected as the Bayes approach provides point estimators based on the loss function whereas the ML method does not incorporate it.

The informative procedure appears to be quite robust even with wrong prior densities. The relative efficieny of the Bayes estimators under the informative prior is larger than of the non-informative prior, regardless of the accuracy of prior information.




### Linear exponential loss function

If a LINEX loss function is used, the Bayes estimator of $\theta$ can be expressed as:
\[
\hat\theta = - \frac{1}{c} ln\, E_\theta\{ exp(-c\theta\}
\]

provided that $E_\theta(.)$ exists and is finite.

The posterior Bayes Estimator, of a LINEX loss function $u = u(exp(-c\alpha), exp(-c\beta))$ is:

\[
\pi^*(\alpha, \beta \mid L_i, R_i) = \frac{\int \int u(exp(-c\alpha), exp(-c\beta)) \pi(\alpha, \beta,L_i, R_i)\, d\alpha\, d\beta}{\int \int \pi(\alpha, \beta, L_i, R_i)\, d\alpha\, d\beta}
\]

where the variable $c$ in function $u(.)$ is the LINEX loss parameter. According to Calabria & Pulcini (1996), $c=\pm0.7$ is used.

The parameters $\hat\alpha$ and $\hat\beta$ can be estimated similarly under the Lindley approximation with the following changes on $u(.)$:

\[
u(\alpha) = e^{-c\alpha},\ u(\beta) = e^{-c\beta},\\
u_1 = -ce^{-c\alpha},\ u_{11} = c^2e^{-c\alpha},\\
u_2 = -ce^{-c\beta}, u_{22} = c^2e^{-c\beta}
\]

Under the Tierney & Kadane procedure, only the $l^*(\alpha^*\beta^*)$ has $u(.)$ which can be expressed as:

\[
l^*(\alpha^*, \beta^*) = l(\alpha, \beta) + \frac{1}{n} ln\,u(exp(-c\alpha),\ exp(-c\beta))
\]
