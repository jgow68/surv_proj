---
title: Summary of Bayesian statistical inference of the loglogistic model with interval-censored
  lifetime data
output:
  pdf_document: default
  html_document: default
header includes: \usepackage{amsmath, amsthm, amssymb}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

## Summary

We try to apply Bayesian approach by employing numerical apporximations from Lindley (1980) and Tierney & Kadane (1986) on survival data which are considered interval-censored. We have considered only a parametric approach assuming the survival data follows a loglogistic distribution model.


### Loglogistic information

Cumulative distribution function


\begin{align}
F(t, \alpha, \beta) = \Big[1 + \bigg(\frac{t}{\alpha}\bigg)^{-\beta}\Big]^{-1} 
\end{align}


Probability density function

$$ f(t, \alpha, \beta) = \frac{\beta}{\alpha}\bigg(\frac{t}{\alpha}\bigg)^{\beta-1}\Big[1 + \bigg(\frac{t}{\alpha}\bigg)^{-\beta}\Big]^{-2} $$

### Bayesian estimation of the unknown parameters

Suppose we have some information on the parameters $\alpha$ and $\beta$ as priori. since both parameters of hte loglogistic model are positive and greater than zero, we assume $\alpha$ and $\beta$ have the following gamma prior distributions:

$$ \pi(\alpha) \propto \alpha^{\ a-1}exp(-\alpha\ b) $$
$$ \pi_{1}(\beta) \propto \beta^{\ c-1}exp(-\beta\ d) $$

The parameters $a, b, c, d$ are assumed to be known and greater than zero based on a prior information available that fits the gamma distribution which is mainly subjective to the investigator. When $a = b = c = d = 0$, then we have non informative priors.

let $[L_{i}\ ,\ R_{i}]$ denote the interval censored data and $T$ represent the unknown time, i.e. $L_{i} \leq T_{i} \leq R_{i}$, where $L_{i}$ is the last inspection time, $R_{i}$ the state end time.

If censoring occurs non-informatively and if the law governing $L_{i}$ and $R_{i}$ doesn't involve any of the parameters of interest / independent of $\alpha$ and $\beta$, we can base our inferences on the likelihood function $L(\alpha,\ \beta;\ L_{i},\ R_{i})$

$$ L(L_{i},\ R_{i} \mid \alpha,\ \beta)= \prod_{i=1}^n \big[F(R_{i}, \alpha, \beta)\ - F(L_{i}, \alpha, \beta) \big] $$

we suppose $[L_{i}, R_{i}]$ is a random sample from loglogistic($\alpha,\ \beta$), then the observed data are given as

$$ L(L_{i},\ R_{i} \mid \alpha,\ \beta) = \prod_{i=1}^n \Bigg( \frac{1}{\big[1+(\frac{L_{i}}{\alpha})^{-\beta}\big]} - \frac{1}{\big[1+(\frac{R_{i}}{\alpha})^{-\beta}\big]} \Bigg)$$
Taking natural log, we get 

$$ l(data \mid \alpha, \beta) = \sum_{i=1}^n ln \Bigg( \frac{1}{\big[1+(\frac{L_{i}}{\alpha})^{-\beta}\big]} - \frac{1}{\big[1+(\frac{R_{i}}{\alpha})^{-\beta}\big]} \Bigg) $$

the joint density function of the data, $\alpha$ and $\beta$ can be obtained as 

$$ \pi(\alpha,\ \beta,\ L_{i},\ R_{i}) \propto l(data \mid \alpha,\ \beta) \pi(\alpha)\pi_{1}(\beta) $$

the posterior density function of $\alpha$ and $\beta$ given the data under squared error loss function, $u(\alpha,\ \beta)$ is 

Let $\delta = (\alpha, \beta)$ then the squared error loss function $L(\delta, \hat{\delta}) = (\delta - \hat{\delta})^2$

$$ \pi^{*}(\alpha,\ \beta,\ L_{i},\ R_{i}) = \frac{\int_0^\infty \int_0^\infty u(\alpha,\ \beta)\ \pi(\alpha)\ \pi_{1}(\beta)\ l(data \mid \alpha, \beta) d\alpha\ d\beta}{\int_0^\infty \int_0^\infty  \pi(\alpha)\ \pi_{1}(\beta)\ l(data \mid \alpha, \beta) d\alpha\ d\beta} $$

The integral is difficult to compute analytically, hence we use two numerical apporximations suggested by Lindley and Tierney & Kadane to estimate the parameters.

### Lindley (1980) numerical approximation approachs

To approximate the ratio of integrals in Equation 7, Lindley proposed the following

posterior SELF-Bayes estimator of an arbitrary function $u(\delta)$ is

\[
E((\theta) \mid x) = \frac{\int w(\theta)\, exp[l(\theta)]\, d\theta}{\int v(\theta)\, exp[l(\theta)]\, d\theta}
\]

where $l(\theta)$ is the log-likelihood and $w(\theta),\ v(\theta)$ are arbitrary functions of $\theta$. $v(\theta)$ is the prior distribution for $\theta$ and $w(\theta) = u(\theta).v(\theta)$. $u(\theta)$ being the function of interest.

then the above posterior expectation for $u(\theta)$ can be shown as

\[ 
E\big(u(\theta) \mid t \big) = \frac{\int u(\theta)\, exp[l(\theta) + \rho(\theta)] d\theta}{\int exp[l(\theta) + \rho(\theta)] d\theta}
\]

where $\rho(\theta) = log[v(\theta)]$

we will approximate the $E\big(u(\theta) \mid t \big)$ asymptotically by

\[
E\big(u(\theta) \mid t \big) = \Big[u + \frac{1}{2} \sum_i \sum_j (u_{ij} + 2u_{i}\rho_{j})\, \delta_{ij} + \frac{1}{2} \sum_i \sum_j \sum_k \sum_l l_{ijk}\, \delta_{ij}\, \delta_{kl}\, u_l \Big]
\]

where $i, j, k, l = 1,2, \dots, n$ and $\theta = (\theta_1, \theta_2, \dots, \theta_m)$ is the vector of the unknow model parameters

the approximate Bayes estimator of $\alpha$ and $\beta$ under the squared error loss function can be summarised as below
\[
\hat{\alpha} = \hat{\alpha} + 0.5(u_{11}\, \delta_{11}) + (u_1\, \rho_1\, \delta_{11}) + 0.5(l_{30}\, u_1\, (\delta_{11})^2)\\
\hat{\beta} = \hat{\beta} + 0.5(u_{22}\, \delta_{22}) + (u_2\, \rho_2\, \delta_{22}) + 0.5(l_{03}\, u_2\, (\delta_{22})^2)
\]

where
\[
\rho = ln\big(\pi(\alpha)\big) + ln\big(\pi_1(\beta)\big)\\
\rho_1 = \frac{d\rho}{\alpha}= \frac{a-1}{\alpha} - b,\quad \rho_2 = \frac{d\rho}{\beta}= \frac{c-1}{\beta} - d\\
u(\alpha)= \alpha,\quad u_1 = \frac{du}{d\alpha}=1,\quad u_{11}=\frac{d^2u}{d\alpha^2}\\
u(\beta)= \beta,\quad u_2 = \frac{du}{d\beta}=1,\quad u_{22}=\frac{d^2u}{d\beta^2}
\]

Now if we let

\[
x=\bigg[1+\bigg(\frac{L_i}{\alpha}\bigg)^\beta \bigg],\quad y=\bigg[1+\bigg(\frac{R_i}{\alpha}\bigg)^\beta \bigg]\\
\delta_{11}=(-l_{20})^{-1},\quad \delta_{22}=(-l_{02})^{-1}\quad where\\
l_{20} = \frac{d^2l}{d\alpha^2},\quad l_{02}=\frac{d^2l}{d\beta^2}
\]

Then we can estimate the approximate Bayes estimator of $\hat{\alpha}$ and $\hat\beta$ accordingly

### Tierney & Kadane (1986) numerical approximation approach

As Lindley's procedure requires to derive the thrid order derivative of the log likelihood function, we may consider an alternative by Tierney & Kadane which only requires the second order derivative for evaluating the ratio of integrals.

we let $v(\alpha, \beta)$ be the prior distribution and $\pi(\alpha,\, \beta \mid L_i,\, R_i)$ be the posterior distribution of $\alpha$ and $\beta$. The Bayes estimate of a function $u(\alpha, \beta)$ under the squared error loss function is the posterior mean:

\[
E(u(\alpha,\, \beta \mid L_i,\,R_i)) = \frac{\int\int exp[nl^*(\alpha^*, \beta^*)])\, d\alpha\, d\beta}{\int\int exp[nl(\alpha, \beta)])\, d\alpha\, d\beta}
\]

where
\[
l(\alpha, \beta) = \frac{log\, v(\alpha, \beta) + l(\alpha, \beta; L_i, R_i)}{n}
\]

and 
\[
l^*(\alpha^*, \beta^*) = \frac{log\, v(\alpha, \beta) + log\, u(\alpha, \beta) + l(\alpha, \beta; L_i, R_i)}{n}
\]


